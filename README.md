# ğŸš€ Mini Datalake Stack

Projeto compacto de Data Lake com Apache Airflow, Apache Spark e MinIO rodando em containers Docker.

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#visÃ£o-geral)
- [Arquitetura](#arquitetura)
- [PrÃ©-requisitos](#prÃ©-requisitos)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [InstalaÃ§Ã£o e ConfiguraÃ§Ã£o](#instalaÃ§Ã£o-e-configuraÃ§Ã£o)
- [Como Usar](#como-usar)
- [Acesso aos ServiÃ§os](#acesso-aos-serviÃ§os)
- [Exemplos](#exemplos)
- [Troubleshooting](#troubleshooting)
- [Comandos Ãšteis](#comandos-Ãºteis)

## ğŸ¯ VisÃ£o Geral

Este projeto fornece uma stack completa de Data Lake com:

- **MinIO**: Armazenamento de objetos S3-compatible (camadas Raw/Bronze/Silver/Gold)
- **Apache Spark**: Processamento distribuÃ­do de dados (Master + Worker)
- **Apache Airflow**: OrquestraÃ§Ã£o de pipelines de dados (Webserver + Scheduler)
- **PostgreSQL**: Banco de metadados do Airflow

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MINI DATALAKE STACK                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Airflow    â”‚    â”‚    Spark     â”‚    â”‚    MinIO     â”‚ â”‚
â”‚  â”‚  Webserver   â”‚â—„â”€â”€â–ºâ”‚    Master    â”‚â—„â”€â”€â–ºâ”‚   Storage    â”‚ â”‚
â”‚  â”‚   :8081      â”‚    â”‚    :8080     â”‚    â”‚ :9000/:9001  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                   â”‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚   Airflow    â”‚    â”‚    Spark     â”‚                     â”‚
â”‚  â”‚  Scheduler   â”‚    â”‚    Worker    â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚         â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚  â”‚  PostgreSQL  â”‚                                         â”‚
â”‚  â”‚  (Metadata)  â”‚                                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes

| ServiÃ§o | DescriÃ§Ã£o | Porta(s) |
|---------|-----------|----------|
| **MinIO** | Object Storage S3-compatible | 9000 (API), 9001 (Console) |
| **Spark Master** | Coordenador do cluster Spark | 8080 (UI), 7077 (Master) |
| **Spark Worker** | NÃ³ worker para processamento | - |
| **Airflow Webserver** | Interface web do Airflow | 8081 |
| **Airflow Scheduler** | Agendador de DAGs | - |
| **PostgreSQL** | Banco de metadados | 5432 |

## ğŸ“¦ PrÃ©-requisitos

- **Docker**: versÃ£o 20.10 ou superior
- **Docker Compose**: versÃ£o 2.0 ou superior
- **Recursos mÃ­nimos**:
  - 8GB RAM
  - 4 CPU cores
  - 20GB espaÃ§o em disco

### Verificar instalaÃ§Ã£o

```bash
docker --version
docker-compose --version
docker info
```

## ğŸ“ Estrutura do Projeto

```
mini-datalake-stack/
â”œâ”€â”€ README.md                 # Este arquivo
â”œâ”€â”€ docker-compose.yml        # DefiniÃ§Ã£o dos serviÃ§os
â”œâ”€â”€ .env                      # VariÃ¡veis de ambiente
â”œâ”€â”€ .gitignore               # Arquivos ignorados pelo git
â”œâ”€â”€ requirements.txt          # DependÃªncias Python
â”œâ”€â”€ startup.sh               # Script para iniciar todos os serviÃ§os
â”œâ”€â”€ shutdown.sh              # Script para parar todos os serviÃ§os
â”œâ”€â”€ restart.sh               # Script para reiniciar serviÃ§os
â”‚
â”œâ”€â”€ config/                   # Arquivos de configuraÃ§Ã£o
â”‚   â””â”€â”€ spark-defaults.conf  # ConfiguraÃ§Ã£o do Spark
â”‚
â”œâ”€â”€ dags/                     # DAGs do Airflow
â”‚   â””â”€â”€ exemplo_pipeline.py  # DAG de exemplo
â”‚
â”œâ”€â”€ spark-apps/              # AplicaÃ§Ãµes Spark
â”‚   â””â”€â”€ exemplo_spark_job.py # Job Spark de exemplo
â”‚
â”œâ”€â”€ scripts/                 # Scripts auxiliares
â”‚   â”œâ”€â”€ check-status.sh      # Verificar status dos serviÃ§os
â”‚   â””â”€â”€ setup-minio.sh       # Configurar buckets no MinIO
â”‚
â””â”€â”€ data/                    # Dados persistidos (criado automaticamente)
    â”œâ”€â”€ minio/               # Armazenamento MinIO
    â”œâ”€â”€ postgres/            # Dados do PostgreSQL
    â”œâ”€â”€ spark/               # Event logs do Spark
    â””â”€â”€ logs/                # Logs do Airflow
```

## ğŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### Passo 1: Navegue atÃ© o diretÃ³rio do projeto

```bash
cd /home/<usuario-logado>/datalake-air-flow/mini-datalake-stack
```

### Passo 2: DÃª permissÃ£o de execuÃ§Ã£o aos scripts

```bash
chmod +x startup.sh shutdown.sh restart.sh
chmod +x scripts/*.sh
```

### Passo 3: Inicie todos os serviÃ§os

```bash
./startup.sh
```

Este script irÃ¡:
1. âœ… Verificar se o Docker estÃ¡ rodando
2. âœ… Criar os diretÃ³rios de dados necessÃ¡rios
3. âœ… Subir todos os containers
4. âœ… Inicializar o banco de dados do Airflow
5. âœ… Criar o usuÃ¡rio admin do Airflow
6. âœ… Mostrar o status dos serviÃ§os

**Tempo estimado**: 2-3 minutos para primeira execuÃ§Ã£o

### Passo 4: Configure os buckets no MinIO (opcional)

```bash
./scripts/setup-minio.sh
```

Este script cria os buckets padrÃ£o para as camadas do Data Lake:
- `raw` - Dados brutos
- `bronze` - Dados ingeridos
- `silver` - Dados refinados
- `gold` - Dados analÃ­ticos

## ğŸ® Como Usar

### Iniciando o ambiente

```bash
./startup.sh
```

### Parando o ambiente

```bash
./shutdown.sh
```

### Reiniciando os serviÃ§os

```bash
./restart.sh
```

### Verificando status

```bash
./scripts/check-status.sh
# ou
docker-compose ps
```

## ğŸŒ Acesso aos ServiÃ§os

### MinIO Console
- **URL**: http://localhost:9001
- **UsuÃ¡rio**: `minioadmin`
- **Senha**: `minioadmin123`
- **Uso**: Gerenciar buckets e objetos, visualizar dados armazenados

### Spark Master UI
- **URL**: http://localhost:8080
- **Uso**: Monitorar jobs Spark, workers, executores e recursos

### Airflow Web UI
- **URL**: http://localhost:8081
- **UsuÃ¡rio**: `admin`
- **Senha**: `admin`
- **Uso**: Gerenciar DAGs, visualizar logs, monitorar execuÃ§Ãµes

## ğŸ“š Exemplos

### Exemplo 1: Executar DAG de teste

1. Acesse Airflow: http://localhost:8081
2. FaÃ§a login (admin/admin)
3. Encontre a DAG `exemplo_datalake_pipeline`
4. Clique no botÃ£o "Play" para executar
5. Acompanhe a execuÃ§Ã£o na interface

### Exemplo 2: Executar job Spark

```bash
# Acessar o container do Spark Master
docker exec -it spark-master bash

# Executar o job de exemplo
/opt/bitnami/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  /opt/spark-apps/exemplo_spark_job.py
```

### Exemplo 3: Acessar MinIO via Python

```python
from minio import Minio

# Criar cliente MinIO
client = Minio(
    "localhost:9000",
    access_key="minioadmin",
    secret_key="minioadmin123",
    secure=False
)

# Listar buckets
buckets = client.list_buckets()
for bucket in buckets:
    print(bucket.name)
```

### Exemplo 4: Criar uma nova DAG

Crie um arquivo em `dags/minha_dag.py`:

```python
from datetime import datetime
from airflow import DAG
from airflow.operators.python import PythonOperator

def minha_funcao():
    print("OlÃ¡ do Airflow!")
    return "Sucesso!"

with DAG(
    'minha_primeira_dag',
    start_date=datetime(2024, 1, 1),
    schedule_interval=None,
    catchup=False
) as dag:
    
    tarefa = PythonOperator(
        task_id='executar_funcao',
        python_callable=minha_funcao
    )
```

A DAG aparecerÃ¡ automaticamente na interface do Airflow em alguns segundos.

## ğŸ”§ Troubleshooting

### Problema: Containers nÃ£o iniciam

```bash
# Verificar logs
docker-compose logs -f

# Verificar recursos do Docker
docker system df
docker system prune  # Limpar recursos nÃ£o utilizados
```

### Problema: Airflow nÃ£o acessa

```bash
# Verificar se o container estÃ¡ rodando
docker-compose ps airflow-webserver

# Ver logs do Airflow
docker-compose logs -f airflow-webserver

# Reiniciar o Airflow
docker-compose restart airflow-webserver
```

### Problema: Spark job falha

```bash
# Ver logs do Spark Master
docker-compose logs -f spark-master

# Ver logs do Spark Worker
docker-compose logs -f spark-worker

# Acessar UI do Spark para detalhes
# http://localhost:8080
```

### Problema: MinIO nÃ£o conecta

```bash
# Verificar container
docker-compose ps minio

# Ver logs
docker-compose logs -f minio

# Testar conectividade
curl http://localhost:9000/minio/health/live
```

### Resetar tudo (CUIDADO: apaga todos os dados)

```bash
docker-compose down -v
rm -rf data/
./startup.sh
```

## ğŸ“ Comandos Ãšteis

### Docker Compose

```bash
# Ver logs de todos os serviÃ§os
docker-compose logs -f

# Ver logs de um serviÃ§o especÃ­fico
docker-compose logs -f airflow-webserver

# Listar containers
docker-compose ps

# Acessar shell de um container
docker exec -it <container-name> bash

# Ver uso de recursos
docker stats
```

### Airflow

```bash
# Acessar CLI do Airflow
docker exec -it airflow-webserver bash
airflow dags list
airflow tasks list <dag_id>
airflow dags trigger <dag_id>

# Testar uma task
airflow tasks test <dag_id> <task_id> 2024-01-01
```

### Spark

```bash
# Submit job Spark
docker exec -it spark-master spark-submit \
  --master spark://spark-master:7077 \
  /opt/spark-apps/seu_job.py

# Spark Shell interativo
docker exec -it spark-master spark-shell

# PySpark interativo
docker exec -it spark-master pyspark
```

### MinIO

```bash
# Usar MinIO Client (mc)
docker run --rm --network mini-datalake-stack_datalake-network \
  minio/mc:latest \
  mc alias set myminio http://minio:9000 minioadmin minioadmin123

# Listar buckets
docker run --rm --network mini-datalake-stack_datalake-network \
  minio/mc:latest \
  mc ls myminio
```

## ğŸ” Credenciais PadrÃ£o

| ServiÃ§o | UsuÃ¡rio | Senha |
|---------|---------|-------|
| Airflow | admin | admin |
| MinIO | minioadmin | minioadmin123 |
| PostgreSQL | airflow | airflow |

**âš ï¸ IMPORTANTE**: Altere as credenciais padrÃ£o em produÃ§Ã£o!

## ğŸ¯ PrÃ³ximos Passos

1. **Personalize as configuraÃ§Ãµes** no arquivo `.env`
2. **Crie suas prÃ³prias DAGs** no diretÃ³rio `dags/`
3. **Desenvolva jobs Spark** no diretÃ³rio `spark-apps/`
4. **Configure camadas do Data Lake** no MinIO (Raw, Bronze, Silver, Gold)
5. **Implemente pipelines de dados** completos

## ğŸ“– ReferÃªncias

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [MinIO Documentation](https://min.io/docs/)
- [Docker Compose Documentation](https://docs.docker.com/compose/)

## ğŸ“„ LicenÃ§a

Este projeto Ã© fornecido "como estÃ¡" para fins educacionais e de desenvolvimento.

---

**Desenvolvido para facilitar o aprendizado e desenvolvimento de Data Lakes** ğŸš€

Para dÃºvidas ou melhorias, consulte a documentaÃ§Ã£o oficial de cada ferramenta.
